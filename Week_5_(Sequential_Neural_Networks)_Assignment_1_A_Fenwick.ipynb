{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_5_(Sequential_Neural_Networks)_Assignment_1_A_Fenwick.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH6d0ePqw3vc"
      },
      "source": [
        "# Developing a neural network from scratch\n",
        "\n",
        "by Ali Fenwick\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, I will develop a neural net from scratch to illustrate the operations of the model (such as forward pass) and explain how neural networks optimize their prediction accuracy through a concept called backpropagation (Rumelhart et al., 1986a). This notebook is part of an assignment which explains the mathematics behind forward pass and backpropagation and aims to reflect how some of the equations are translated into code to allow the model to run effectively. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "A dataset has been provided by the examiner for use in this assignment to test the performance of the neural net. The dataset contains 1000 rows and 4 columns. 3 columns x variables which forms the input data for the model and 1 column which is the output variables Y which I aim to predict with the neural net. The Y variable consists of 0s and 1s, which means that the I will address a binary classification problem. Besides testing the dataset with the coded neural net, I will also us a logistic regression algorithm from sklearn to compare the results between both approaches.\n",
        "\n",
        "## Forward Pass and Backpropagation.\n",
        "\n",
        "In machine learning, forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network going from the input layer to the output layer. Backpropagation, on the other hand, computes the gradient of the loss function with respect to the weights of the network for a single inputâ€“output example. This is an important step in neural networks, as this is the way the algorithm learns to correct the weights and biases for its model. Backpropagation evaluates the expression for the derivative of the cost function (following the chain rule) as a product of derivatives between each layer from right to left with the gradient of the weights between each layer being a simple modification of the partial products. \n",
        "\n",
        "# Coding Section\n",
        "\n",
        "## The development of a neural network focusing on forward and backward propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X4_5_SdjSVS"
      },
      "source": [
        "Let's start with loading the necessry libraries for the model development. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtibMHgLxASt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuqICedUkH2J"
      },
      "source": [
        "Here I will load the dataset and explore the data briefly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y-SOpEZxEG4"
      },
      "source": [
        "data = pd.read_csv(\"/content/assignment1_SeqNNs.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjGgz6c7xKAl",
        "outputId": "b71b240e-cb5b-4146-a8eb-059366b53f78"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   x1      1000 non-null   float64\n",
            " 1   x2      1000 non-null   float64\n",
            " 2   x3      1000 non-null   float64\n",
            " 3   y       1000 non-null   float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 31.4 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "1NtZiXOJxMLx",
        "outputId": "b855da9d-b4a3-4298-aff1-2224cc1aeb0f"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.013503</td>\n",
              "      <td>1.019268</td>\n",
              "      <td>0.533061</td>\n",
              "      <td>0.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.032161</td>\n",
              "      <td>0.999535</td>\n",
              "      <td>1.019132</td>\n",
              "      <td>0.499092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-2.973315</td>\n",
              "      <td>-2.209955</td>\n",
              "      <td>-2.519478</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.674494</td>\n",
              "      <td>0.327711</td>\n",
              "      <td>-0.162202</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.029750</td>\n",
              "      <td>1.012551</td>\n",
              "      <td>0.488647</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.669901</td>\n",
              "      <td>1.688176</td>\n",
              "      <td>1.193795</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.695300</td>\n",
              "      <td>3.929661</td>\n",
              "      <td>4.357940</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x1           x2           x3            y\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000\n",
              "mean      0.013503     1.019268     0.533061     0.466000\n",
              "std       1.032161     0.999535     1.019132     0.499092\n",
              "min      -2.973315    -2.209955    -2.519478     0.000000\n",
              "25%      -0.674494     0.327711    -0.162202     0.000000\n",
              "50%       0.029750     1.012551     0.488647     0.000000\n",
              "75%       0.669901     1.688176     1.193795     1.000000\n",
              "max       3.695300     3.929661     4.357940     1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4qyHRgtwhSl"
      },
      "source": [
        "# Developing a class for the neural net\n",
        "\n",
        "Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.\n",
        "\n",
        "In this class, I will define the parameters such has the amount of hidden layers, regularization, training examples, learning rate, and batch side. After which, the class will reflect the one-hot encoding to define the Y output, sigmoid activation necessary to compute the output, the forward pass, the cost function, back propagation, and the gradient descent. The class also reflects the mathematical equations required for these operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOkI_sFO5kWU"
      },
      "source": [
        "class NeuralNetMLP(object):\n",
        "    \n",
        "    \"\"\" Defining a class for deep learning model for a binanry classification \n",
        "    problem. Initializing the class with the following parameters: number of \n",
        "    hidden layers, epochs, and batch size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=1, epochs=150, eta=0.001, seed=None, shuffle=True, \n",
        "                  minibatch_size=1):\n",
        "        \n",
        "        self.n_hidden = n_hidden\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.random = np.random.RandomState(seed)\n",
        "        self.shuffle = shuffle\n",
        "        self.minibatch_size = minibatch_size\n",
        "            \n",
        "    # defining function for the sigmoid equation - (Ïƒ): g(z) = 1 / (1 + e^{-z}). \n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "            return 1. / (1. + np.exp(-z))\n",
        "\n",
        "        # defining a function for the forward pass\n",
        "\n",
        "    def _forward(self, X):\n",
        "\n",
        "            # Forward Pass\n",
        "            # Stepwise approach starting with the net input (z) of the hidden layer. \n",
        "            # Next set the activation (a_h) of the hidden layer using the signoid function\n",
        "            # (alternatively tanh can be used here like in the written assignment). \n",
        "            # Next step is the net input of the output layer (z_out) and finally the activation\n",
        "            # of the output layer (a_out) using the signmoid function\n",
        "\n",
        "        z_h = np.dot(X, self.w_h) + self.b_h\n",
        "        a_h = self._sigmoid(z_h)\n",
        "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "        a_out = self._sigmoid(z_out)\n",
        "        return z_h, a_h, z_out, a_out\n",
        "\n",
        "            #Loss Function\n",
        "            #After the forward pass we define the loss function Mean Square Error (MSE).\n",
        "            #The left and right part of the loss function are calculated seperately.\n",
        "\n",
        "    def _cost_function(self, y_real, output):\n",
        "            \n",
        "        left = -y_real * (np.log(output))\n",
        "        right = (1. - y_real) * np.log(1. - output)\n",
        "        cost = np.sum(left - right)\n",
        "        return cost\n",
        "\n",
        "    def predict(self, X):\n",
        "        z_h, a_h, z_out, a_out = self._forward(X)\n",
        "        y_pred = np.argmax(z_out, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
        "            \n",
        "        #Using the training data to update the weights\n",
        "        n_output = np.unique(y_train).shape[0]  \n",
        "        n_features = X_train.shape[1]\n",
        "\n",
        "        # weights for input -> hidden\n",
        "        self.b_h = np.zeros(self.n_hidden)\n",
        "        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n",
        "                                      size=(n_features, self.n_hidden))\n",
        "\n",
        "        # weights for hidden -> output\n",
        "        self.b_out = np.zeros(n_output)\n",
        "        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n",
        "                                        size=(self.n_hidden, n_output))\n",
        "\n",
        "        epoch_strlen = len(str(self.epochs))  # for progress formatting\n",
        "        self.eval_ = {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
        "\n",
        "        y_train_real = n_output\n",
        "\n",
        "        # iterate over training epochs\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            # iterate over minibatches\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "\n",
        "            if self.shuffle:\n",
        "                self.random.shuffle(indices)\n",
        "\n",
        "            for start in range(0, indices.shape[0] - self.minibatch_size +\n",
        "                                    1, self.minibatch_size):\n",
        "                batch = indices[start:start + self.minibatch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                z_h, a_h, z_out, a_out = self._forward(X_train[batch])\n",
        "\n",
        "                \"\"\"Backpropagation algorithm adjusting the weights (delta) to find\n",
        "                the correct measurements for the model. This is done by calculating \n",
        "                the partial derivative of the loss function over the weights using\n",
        "                the chain rule.\"\"\"\n",
        "\n",
        "                # Calculating delta_h combining the delta out error (left-side of the equation \n",
        "                #of expression #4) together with the activation of the derivative \n",
        "                # (right-hand side of expression #4 on page 412). First we calculate with delta\n",
        "                #output error (a_out is calculated Y) then we calculate the derivative of the\n",
        "                #activation function. \n",
        "                \n",
        "                delta_out = a_out - y_train_real #Step 1\n",
        "                sigmoid_der_h = a_h * (1. - a_h) #Step 2\n",
        "                delta_h = (np.dot(delta_out, self.w_out.T) *\n",
        "                            sigmoid_der_h) #Step 3\n",
        "                \n",
        "                #Note: sigmoid derivative code represents the compact form of the \n",
        "                #derivative of the activation function from Luis' ppt 02\n",
        "              \n",
        "\n",
        "                #Computing Gradient Descent\n",
        "\n",
        "                # Computing the loss gradient for the output layer in expression #1\n",
        "                # of the book (page 412)\n",
        "                grad_w_out = np.dot(a_h.T, delta_out)\n",
        "                grad_b_out = np.sum(delta_out, axis=0)\n",
        "\n",
        "                # Computing the loss gradient for the hidden layer in expression #5 \n",
        "                # of the book (page 412)\n",
        "                grad_w_h = np.dot(X_train[batch].T, delta_h)\n",
        "                grad_b_h = np.sum(delta_h, axis=0)\n",
        "\n",
        "                # Updating weights for both the hidden layer and the outer later. First,\n",
        "                #let's calculate the hidden layer (see assignment 1 equations)\n",
        "                delta_w_h = grad_w_h + self.w_h\n",
        "                delta_b_h = grad_b_h \n",
        "                self.w_h -= self.eta * delta_w_h # updating weights of hidden layer\n",
        "                self.b_h -= self.eta * delta_b_h # updating bias of hidden layer\n",
        "\n",
        "                delta_w_out = grad_w_out + self.w_out \n",
        "                delta_b_out = grad_b_out  \n",
        "                self.w_out -= self.eta * delta_w_out # updating weights of the output layer\n",
        "                self.b_out -= self.eta * delta_b_out # updating nias of the output layer\n",
        "\n",
        "\n",
        "             # Model Evaluation\n",
        "\n",
        "            # During training evaluate each epoch\n",
        "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
        "            \n",
        "            cost = self._cost_function(y_real=y_train_real,\n",
        "                                      output=a_out)\n",
        "\n",
        "            train_y_pred = self.predict(X_train)\n",
        "            valid_y_pred = self.predict(X_valid)\n",
        "\n",
        "            train_accuracy = ((np.sum(y_train == train_y_pred)).astype(np.float) /\n",
        "                          X_train.shape[0])\n",
        "            valid_accuracy = ((np.sum(y_valid == valid_y_pred)).astype(np.float) /\n",
        "                          X_valid.shape[0])\n",
        "\n",
        "            import sys\n",
        "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f '\n",
        "                              '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
        "                              (epoch_strlen, i+1, self.epochs, cost,\n",
        "                              train_accuracy*100, valid_accuracy*100))\n",
        "            sys.stderr.flush()\n",
        "\n",
        "            self.eval_['cost'].append(cost)\n",
        "            self.eval_['train_acc'].append(train_accuracy)\n",
        "            self.eval_['valid_acc'].append(valid_accuracy)\n",
        "\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDJ5hD5nl9OX"
      },
      "source": [
        "In this section, I will divide the dataset into a training and test set. 80% of the data will be used to train the model and 20% of the data will be used to test the model once trained. The model will run 200 training examples to evaluate the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPI3lD2Ql3iO"
      },
      "source": [
        "n_epochs = 200\n",
        "\n",
        "# for the linear classifier\n",
        "train_x = data.values[:800,:-1]\n",
        "train_y = data.values[:800,-1]\n",
        "\n",
        "test_x = data.values[800:,:-1]\n",
        "test_y = data.values[800:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KskotbrmqUE"
      },
      "source": [
        "Here the model is fitted and runs on both the test and validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DNW014Il9T9",
        "outputId": "b682dae3-6785-4a36-bba8-bdf835322b82"
      },
      "source": [
        "nn = NeuralNetMLP(n_hidden=1,  \n",
        "                  epochs=n_epochs, \n",
        "                  eta=0.001,\n",
        "                  minibatch_size=100, \n",
        "                  shuffle=True,\n",
        "                  seed=1)\n",
        "\n",
        "nn.fit(train_x, train_y, test_x, test_y)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200/200 | Cost: 278.53 | Train/Valid Acc.: 94.88%/95.00% "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NeuralNetMLP at 0x7f61df596310>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2kHHZ5OnCLj"
      },
      "source": [
        "Model performance is 94.88% for the training set and 95.00% for the validtion set which shows good model fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1z3TQwTp4AC"
      },
      "source": [
        "# Testing the dataset with sklearn's LogisticRegression() algorithm and comparimg the results with the neural net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDcttDR1nhS4"
      },
      "source": [
        "Load necessary librabries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYTK1bf-p1oF"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-cELpemn6K2"
      },
      "source": [
        "Create training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDEV95xZ5FKA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, y_train, X_test, y_test = train_test_split(train_x, train_y, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMAkDePOoCMu"
      },
      "source": [
        "Loading the logistical regression algorithm fitting it with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4RqFtLap27f",
        "outputId": "792ae4ca-c49b-4e00-a5d2-dc5e373c378f"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeJXL7QxoOSO"
      },
      "source": [
        "Training accuracy is 94.4%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJhcKnFb5aIN",
        "outputId": "ebec44d0-db5f-4bc1-87ad-dcdd75000549"
      },
      "source": [
        "logreg.score(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94375"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeBN4T6ZoSsp"
      },
      "source": [
        "Validation set accuracy is 95%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPauY09Y5pBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a86dd17-7928-4d72-e772-28bc21ee6b8c"
      },
      "source": [
        "# Use score method to get accuracy of model\n",
        "score = logreg.score(test_x, test_y)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYsrcGUxK6P",
        "outputId": "c67a852a-953b-4ee4-ce68-077500af18fc"
      },
      "source": [
        "predictions = logreg.predict(test_x)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
            " 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
            " 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVDcg4K2ofDF"
      },
      "source": [
        "Here I also conducted a confusion matrix which shows great results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px-FmaZZVyF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c51dfc6-df4f-4a2e-cc0c-3d90f2470861"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "\n",
        "cm = metrics.confusion_matrix(test_y, predictions)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[100   7]\n",
            " [  3  90]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "s-u_jlUz_rif",
        "outputId": "42e84f59-88b4-459d-8659-05266f37128d"
      },
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFMD3TJw_x4t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWS56HiUDS21"
      },
      "source": [
        "# Summary of Neural Net Design and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WbYEJNcO--W"
      },
      "source": [
        "The design of the neural net in python, without using existing librabries like Keras and Tensorflow, was an interesting exercise. It helped to understand the math underlying forward pass, backpropagation, and gradient descent and how to code these various algoritms. Having a solid understanding of how the mechanics work of neural nets is crucial for model design and improvement.\n",
        "\n",
        "In this notebook, the performance of a neural net was compared with that of a logistical regression algorithm from sklearn using a provided dataset. The model addressed a binary classification problem predicting either 0 or 1 in the output variable Y using three input columns X. Model accuracy and the loss function were measures of performance. Both models performed equally well showing 94.4% - 94.8% training accuracy and 95.0% validation accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-F9r-BDXYe"
      },
      "source": [
        "# Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA9GrgR-ozy5"
      },
      "source": [
        "* This assignment was particularly challenging due to some initial lack in understanding of the derivative calculations. Nevertheless, a very learningful exercise to better understand how things work under the hood compared to using Keras and Tensorflow to design neural networks. This assignment has increased my appreciation for better understanding the math underlying the development of neural networks and the translation of these equations into code. \n",
        "\n",
        "* The neural net was representated in the current format. However, there are many other ways you can code a neural network. There is no perfect script. Parts of the neural net were inspired by creations of other developers and practice code from the book Python Machine Learning (Raschka, S. & Mirjalili, V., 2017). I will familiarize myself more in building neural nets from scratch in the coming months to be able to build such models more independently.\n",
        "\n",
        "* Hyperparameter tuning could have been applied more to find a better loss and accuracy curve. Future trials could improve the overall model performance by further tuning the parameters even though in this case the dataset was not that big. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThVvij3lDZON"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xCTRlSVfP2z"
      },
      "source": [
        "# References\n",
        "\n",
        "Amari, S. (1993). Backpropagation and stochastic gradient descent method. Neurocomputing, 5, 185-196.\n",
        "\n",
        "Brilliant. Backpropagation. Retrieved from https://brilliant.org/wiki/backpropagation/ \n",
        "\n",
        "P. J. Werbos, \"Backpropagation through time: what it does and how to do it,\" in Proceedings of the IEEE, vol. 78, no. 10, pp. 1550-1560\n",
        "\n",
        "Raschka, S., & Mirjalili, V. (2017). Machine learning mit Python und Scikit-Learn und TensorFlow: Das Praxis-Handbuch fÃ¼r Data Science, Predictive Analytics und Deep Learning. MITP Verlags GmbH & Company KG.\n",
        "\n",
        "Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986b). \"8. Learning Internal Representations by Error Propagation\". In Rumelhart, David E.; McClelland, James L. (eds.). Parallel Distributed Processing : Explorations in the Microstructure of Cognition. Volume 1 : Foundations. Cambridge: MIT Press. ISBN 0-262-18120-7.\n",
        "\n",
        "Wikipedia. Backpropagation. Retrieved from https://en.wikipedia.org/wiki/Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypixo2grfSfk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}